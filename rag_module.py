import os
import asyncio
from dotenv import load_dotenv

from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableBranch
from langchain_core.output_parsers import StrOutputParser

# .env íŒŒì¼ì— ì €ì¥ëœ API í‚¤ ë¡œë“œ
load_dotenv()

# =========================
# íŒŒë¼ë¯¸í„° ì¡°ì • (íŒŒì¼ ë‚´ë¶€ ìˆ˜ì • ë°©ì‹)
# =========================
CHUNK_SIZE = 1000  # í•œ ì¡°ê°ì— ë‹´ê¸°ëŠ” ë¬¸ë§¥ì˜ ê¸¸ì´ (ì¶©ë¶„í•œ ë§¥ë½ì„ ë‹´ê¸° ìœ„í•´ í™•ëŒ€)
CHUNK_OVERLAP = 200  # ì²­í¬ë¥¼ ìë¥¼ ë•Œ ê²¹ì¹˜ëŠ” ë¶€ë¶„ (ë¬¸ë§¥ ë‹¨ì ˆ ìµœì†Œí™” + ì—°ê²°ì„± ê°•í™”)

RETRIEVER_K = 10  # ìµœì¢…ì ìœ¼ë¡œ LLMì— ë„£ì„ ì²­í¬ ê°œìˆ˜ (ë” í’ë¶€í•œ ê·¼ê±°)
RETRIEVER_FETCH_K = 60  # í›„ë³´ë¡œ ë” ë§ì´ ë½‘ì•„ë†“ê³  ê·¸ì¤‘ì—ì„œ ë‹¤ì–‘í•˜ê²Œ ê³ ë¥´ëŠ” í­
RETRIEVER_LAMBDA = 0.6  # ìœ ì‚¬ë„ vs ë‹¤ì–‘ì„± ê· í˜• ì¡°ì • (ë” ë†’ì€ ìœ ì‚¬ë„ ë¹„ì¤‘)

MODEL_NAME = "gpt-4o"
TEMPERATURE = 0.1  # ì•½ê°„ì˜ ì°½ì˜ì„±ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í‘œí˜„
# =========================


def format_docs_with_pages(docs_list):
    blocks = []
    for d in docs_list:
        page = d.metadata.get("page", None)
        # ì‚¬ëŒì´ ë³´ê¸° ì¢‹ê²Œ 1ë¶€í„° í‘œê¸°
        page_str = f"{page + 1}" if isinstance(page, int) else "?"
        text = (d.page_content or "").strip()
        if not text:
            continue
        blocks.append(f"[p.{page_str}]\n{text}")
    return "\n\n---\n\n".join(blocks)


async def retrieve_docs_for_queries(retriever, queries: list[str]) -> list:
    async def _retrieve(query: str):
        if hasattr(retriever, "ainvoke"):
            return await retriever.ainvoke(query)
        if hasattr(retriever, "aget_relevant_documents"):
            return await retriever.aget_relevant_documents(query)
        return await asyncio.to_thread(retriever.get_relevant_documents, query)

    tasks = [_retrieve(q) for q in queries]
    results = await asyncio.gather(*tasks)

    seen = set()
    merged = []
    for docs in results:
        for d in docs:
            key = (d.metadata.get("source"), d.metadata.get("page"), d.page_content)
            if key in seen:
                continue
            seen.add(key)
            merged.append(d)
    return merged


def create_rag_chain(pdf_path: str):
    # [1ë‹¨ê³„] ë¬¸ì„œ ë¡œë“œ (Document Load) - PyMuPDFLoaderëŠ” ë³´í†µ í˜ì´ì§€ ë‹¨ìœ„ Documentë¥¼ ë°˜í™˜
    loader = PyMuPDFLoader(pdf_path)
    docs = loader.load()

    # [2ë‹¨ê³„] ë¬¸ì„œ ë¶„í•  (Text Split) - QA/ìš”ì•½(RAG)ìš©
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", " ", ""],
    )
    split_documents = text_splitter.split_documents(docs)

    # [3~4ë‹¨ê³„] ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)

    # [5ë‹¨ê³„] ê²€ìƒ‰ê¸°(Retriever) ìƒì„±
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": RETRIEVER_K,
            "fetch_k": RETRIEVER_FETCH_K,
            "lambda_mult": RETRIEVER_LAMBDA,
        },
    )

    # [6~7ë‹¨ê³„] LLM
    llm = ChatOpenAI(model_name=MODEL_NAME, temperature=TEMPERATURE)

    # -------------------------
    # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…(í˜ì´ì§€ í‘œê¸° í¬í•¨) & Rerank ì ìš©
    # -------------------------
    def extract_question(inp):
        if isinstance(inp, dict):
            return inp.get("question", "")
        return inp

    def has_context(inp):
        return isinstance(inp, dict) and inp.get("context") is not None

    def extract_context(inp):
        return inp.get("context", "")

    # Rerankë¥¼ ì ìš©í•˜ëŠ” í•¨ìˆ˜
    def rerank_and_format(inp):
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ rerankí•œ í›„ í¬ë§·íŒ…"""
        if isinstance(inp, dict):
            query = inp.get("question", "")
            docs = inp.get("docs", [])
        else:
            # inpê°€ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
            query = ""
            docs = inp if isinstance(inp, list) else []
        
        if docs and query:
            # rerank ì ìš©
            reranked_docs = rerank_results(query, docs, llm)
            return format_docs_with_pages(reranked_docs)
        return format_docs_with_pages(docs)

    question_selector = RunnableLambda(extract_question)
    format_docs_runnable = RunnableLambda(format_docs_with_pages)
    
    # ê²€ìƒ‰ í›„ rerankë¥¼ ê±°ì¹˜ëŠ” ì²´ì¸
    def retrieve_with_rerank(inp):
        """ì§ˆë¬¸ì„ ë°›ì•„ retrieverë¡œ ê²€ìƒ‰í•˜ê³  rerank ì ìš©"""
        query = extract_question(inp)
        if hasattr(retriever, "invoke"):
            docs = retriever.invoke(query)
        else:
            docs = retriever.get_relevant_documents(query)
        
        # rerank ì ìš©
        reranked_docs = rerank_results(query, docs, llm)
        return format_docs_with_pages(reranked_docs)
    
    retriever_with_rerank = RunnableLambda(retrieve_with_rerank)
    context_chain = question_selector | retriever_with_rerank
    context_selector = RunnableBranch(
        (has_context, RunnableLambda(extract_context)),
        context_chain,
    )

    
    # 1) QA í”„ë¡¬í”„íŠ¸ (ë¬¸ì„œ ê·¼ê±° ê¸°ë°˜)
    qa_template = """ë‹¹ì‹ ì€ 'ì›”ë“œë¹„ì „ ì‚¬ë‚´ ë¬¸ì„œ' ê¸°ë°˜ Q&A ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ <Context>ì— í¬í•¨ëœ ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ë‹µí•˜ì„¸ìš”. ì™¸ë¶€ì§€ì‹/ì¶”ì¸¡/ì¸í„°ë„· ì •ë³´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

[í•µì‹¬ ê·œì¹™]
1. <Context>ì— ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ë‹µì„ ë§Œë“¤ì§€ ë§ê³  "ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠëŠ” ì‚¬í•­"ì´ë¼ê³  ëª…í™•íˆ ë§í•  ê²ƒ
2. ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì •í™•íˆ ì´í•´í•œ í›„ í•œ ì¤„ë¡œ ì¬êµ¬ì„± ì œì‹œ
3. ë‹µë³€ì€ ì‚¬ìš©ìê°€ ë°”ë¡œ ì´í•´í•˜ê³  í–‰ë™/ê²°ì •í•  ìˆ˜ ìˆë„ë¡ êµ¬ì¡°í™”í•  ê²ƒ
4. ëª¨ë“  ê·¼ê±°ì—ëŠ” p.(í˜ì´ì§€ë²ˆí˜¸)ë¥¼ í¬í•¨í•  ê²ƒ
5. í•µì‹¬ ì •ë³´ ìš°ì„ ìœ¼ë¡œ, í•„ìš”í•œ ê²½ìš°ë§Œ ì„¸ë¶€ì‚¬í•­ ì¶”ê°€
6. ì§ˆë¬¸ì´ ëª¨í˜¸í•œ ê²½ìš° ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ê´€ë ¨ í•­ëª©ë“¤ì„ ì œì‹œ

[ì¶œë ¥ í˜•ì‹ (ì—„ê²©íˆ ì¤€ìˆ˜)]
## ì§ˆë¬¸ ì¬êµ¬ì„±
(ì§ˆë¬¸ì˜ ì‹¤ì œ ëª©ì ê³¼ ì˜ë„ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì •ì˜)

## í•µì‹¬ ë‹µë³€
(ì§ê²°ëœ ë‹µë³€, ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ ë¨¼ì €)

## ìƒì„¸ ì„¤ëª…
(í•„ìš”í•œ ë°°ê²½ì •ë³´, ì¡°ê±´, ì˜ˆì™¸ì‚¬í•­ ë“± - bullet list í˜•ì‹)

## ê·¼ê±° (ì¶œì²˜ ëª…ì‹œ)
- p.ë²ˆí˜¸: (í•´ë‹¹ ë‚´ìš© ìš”ì•½)
(2~4ê°œì˜ í•µì‹¬ ê·¼ê±°)

## í™•ì¸í•˜ë©´ ì¢‹ì€ ì¶”ê°€ ì •ë³´
(ë¬¸ì„œì— ì¡´ì¬í•˜ëŠ” ê´€ë ¨ ì²´í¬í¬ì¸íŠ¸/ì„œë¥˜/ê¸°í•œ ë“±, ìˆìœ¼ë©´ 2~3ê°œ)

---
*â€» ì´ ì„¹ì…˜ì´ ë¹„ì–´ìˆì„ ê²½ìš° ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠëŠ” ì‚¬í•­ì…ë‹ˆë‹¤.*

<Context>
{context}

ì§ˆë¬¸: {question}

í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
"""

    qa_prompt = ChatPromptTemplate.from_template(qa_template)

    qa_chain = (
        {"context": context_selector, "question": question_selector}
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    
    # 2) ìš”ì•½/ì •ë¦¬ í”„ë¡¬í”„íŠ¸ (RAG ê¸°ë°˜ "ë³´ê³ ì„œ" ìš”ì•½)
    summary_template = """ë‹¹ì‹ ì€ ì›”ë“œë¹„ì „ ì‚¬ë‚´ ë¬¸ì„œë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ 'ìš”ì•½Â·ì •ë¦¬Â·ë³´ê³ 'í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ <Context>ì— í¬í•¨ëœ ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. ì™¸ë¶€ì§€ì‹/ì¶”ì¸¡/ì¸í„°ë„· ì •ë³´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
ëŠ” ì‚¬í•­"ì— ëª…ì‹œí•  ê²ƒ
3. ë³´ê³ ì„œì²˜ëŸ¼ ê¹”ë”í•˜ê³  êµ¬ì¡°ì ìœ¼ë¡œ ì‘ì„±
4. ê°€ëŠ¥í•˜ë©´ p.ë²ˆí˜¸ë¥¼ í•¨ê»˜ í‘œê¸°
5. ì‚¬ìš©ìì˜ ìš”ì²­ ê´€ì ì— ë§ì¶° í•µì‹¬ ì •ë³´ ìš°ì„  ì •ë ¬

[ì¶œë ¥ í˜•ì‹ (ì—„ê²©íˆ ì¤€ìˆ˜)]
## ğŸ“‹ í•µì‹¬ ìš”ì•½ (3~5ê°œ)
- (ê°€ì¥ ì¤‘ìš”í•œ ë‚´ìš©ë¶€í„° ë‹¨ê³„ë³„ë¡œ)

## ğŸ“Š ìƒì„¸ ì •ë¦¬
(ì‚¬ìš©ì ìš”ì²­ ê´€ì ìœ¼ë¡œ ì²´ê³„ì  ì •ë¦¬)

## ğŸ” ë¬¸ì„œ ê¸°ë°˜ ê·¼ê±°
- p.ë²ˆí˜¸: (ë‚´ìš©ìš”ì•½)
(2~5ê°œì˜ ì£¼ìš” ê·¼ê±°)

---
*â€» ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠëŠ” ì‚¬í•­: *
(ì—†ìœ¼ë©´ 'ì—†ìŒ')

<Context>
{context}

ì‚¬ìš©ì ìš”ì²­: {question}

í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
"""
    summary_prompt = ChatPromptTemplate.from_template(summary_template)

    summary_chain = (
        {"context": context_selector, "question": question_selector}
        | summary_prompt
        | llm
        | StrOutputParser()
    )

    
    # 3) í˜ì´ì§€ë³„ ìš”ì•½ ëª¨ë“œ (read-all, ëˆ„ë½ ìµœì†Œí™”)
    page_prompt = ChatPromptTemplate.from_template("""ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ ë¬¸ì„œì˜ 'í•´ë‹¹ í˜ì´ì§€'ë§Œ ì •í™•íˆ ìš”ì•½í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì´ í˜ì´ì§€ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì“°ì§€ ë§ˆì„¸ìš”. ì™¸ë¶€ì§€ì‹/ì¶”ì¸¡/ì¸í„°ë„· ì •ë³´ë„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

[ì¶œë ¥ í˜•ì‹]
## í•µì‹¬ ìš”ì§€ (3ê°€ì§€)
1. (ê°€ì¥ ì¤‘ìš”í•œ ë‚´ìš©)
2. (ë¶€ê°€ ì •ë³´)
3. (ì£¼ì˜/ì˜ˆì™¸ì‚¬í•­)

## ì¤‘ìš” ê·œì •Â·ì ˆì°¨Â·ìˆ˜ì¹˜Â·ì£¼ì˜ì‚¬í•­
- í•­ëª© 1
- í•­ëª© 2
(ìˆìœ¼ë©´ 2~4ê°œ)

í˜ì´ì§€ ë‚´ìš©:
{page_text}

í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
""")

    def summarize_pages_to_text(_question: str) -> str:
        """docs(í˜ì´ì§€ ë‹¨ìœ„)ë¥¼ ìˆœíšŒí•˜ë©° p.ë³„ ìš”ì•½ì„ ë§Œë“¤ê³  ë¬¸ìì—´ë¡œ í•©ì¹¨"""
        items = []
        for d in docs:
            page = d.metadata.get("page", None)
            page_no = page + 1 if isinstance(page, int) else None

            text = (d.page_content or "").strip()
            if not text:
                continue

            summary = llm.invoke(page_prompt.format(page_text=text)).content
            if page_no is None:
                items.append((10**9, summary))
            else:
                items.append((page_no, summary))

        items.sort(key=lambda x: x[0])

        out_lines = []
        for p, s in items:
            if p == 10**9:
                out_lines.append("## p.?\n" + s.strip())
            else:
                out_lines.append(f"## p.{p}\n{s.strip()}")
        return "\n\n".join(out_lines)

    pagewise_chain = RunnableLambda(summarize_pages_to_text)

    # -------------------------
    # ëª¨ë“œ ê°ì§€/ë¼ìš°íŒ…
    # -------------------------
    SUMMARY_HINTS = ("ìš”ì•½", "ì •ë¦¬", "ë³´ê³ ", "ë¦¬í¬íŠ¸", "ê°œìš”", "í•µì‹¬", "ì „ë°˜", "ì „ì²´", "êµ¬ì¡°", "ëª©ì°¨")
    PAGEWISE_HINTS = ("í˜ì´ì§€ë³„", "page by page", "í˜ì´ì§€ ë‹¨ìœ„", "ìª½ë³„", "pë³„")

    def route(question_or_input) -> str:
        q = extract_question(question_or_input)
        q = (q or "").strip().lower()
        if any(k in q for k in PAGEWISE_HINTS):
            return "pagewise"
        if any(k in q for k in SUMMARY_HINTS):
            return "summary"
        return "qa"

    def is_pagewise(q: str) -> bool:
        return route(q) == "pagewise"

    def is_summary(q: str) -> bool:
        return route(q) == "summary"

    rag_chain = RunnableBranch(
        (is_pagewise, pagewise_chain),
        (is_summary, summary_chain),
        qa_chain,  # default
    )

    return rag_chain, retriever

# =========================
# ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =========================

def query_expansion(query: str) -> list[str]:
    """
    ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ê´€ì ì—ì„œ ì¬êµ¬ì„±í•˜ì—¬
    ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤. (Hybrid Search ê¸°ì´ˆ)
    """
    llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0.7)
    
    expansion_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì¬êµ¬ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì›ë³¸ ì§ˆë¬¸: {query}

ì•„ë˜ 5ê°€ì§€ ê´€ì ì—ì„œ ê°ê° ë‹¤ì‹œ ì“°ì„¸ìš”. í•œ ì¤„ì”©ë§Œ.
1. ì§ì„¤ì  í‘œí˜„ (ì›ë¬¸ê³¼ ê±°ì˜ ê°™ë˜, ë” ëª…í™•í•˜ê²Œ)
2. ê´€ë ¨ ê°œë… í¬í•¨ (ë™ì˜ì–´, ìœ ì‚¬ ê°œë…ì„ ì„ì–´ì„œ)
3. ë°°ê²½/ë§¥ë½ ê°•í™” (why/howë¥¼ í¬í•¨í•´ì„œ)
4. ì‹¤ë¬´ ê´€ì  (ì‹¤ì œ ì—…ë¬´ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ì“°ì¼ì§€)
5. ì—­ì§ˆë¬¸ (í•µì‹¬ ì˜ë„ë¥¼ ì—­ìœ¼ë¡œ í‘œí˜„)

í˜•ì‹: ë§ˆí¬ë‹¤ìš´ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ë¡œ, ê° í•­ëª©ì€ í•œ ì¤„ì”©ë§Œ ì œê³µí•˜ì„¸ìš”.
""")
    
    chain = expansion_prompt | llm | StrOutputParser()
    result = chain.invoke({"query": query})
    
    # ì‘ë‹µì„ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±
    lines = [line.strip() for line in result.split('\n') if line.strip() and line[0].isdigit()]
    queries = [line.split('. ', 1)[-1] if '. ' in line else line for line in lines]
    
    return [query] + queries[:4]  # ì›ë³¸ + 4ê°€ì§€


def rerank_results(query: str, retrieved_docs: list, llm=None) -> list:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±ìœ¼ë¡œ ì¬ì •ë ¬í•©ë‹ˆë‹¤.
    (ë‹µë³€ í’ˆì§ˆ í–¥ìƒ)
    """
    if llm is None:
        llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0)
    
    if not retrieved_docs:
        return retrieved_docs
    
    rerank_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§ˆë¬¸: {query}

ì•„ë˜ ë¬¸ì„œë“¤ì„ ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ë„ë¡œ ì •ë ¬í•˜ì„¸ìš”.
ê°€ì¥ ê´€ë ¨ë„ ë†’ì€ ê²ƒë¶€í„° ìˆœì„œëŒ€ë¡œ ì¸ë±ìŠ¤ë§Œ ì œê³µí•˜ì„¸ìš”. (ì˜ˆ: 2, 0, 3, 1)

{docs_text}
""")
    
    # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¤€ë¹„
    docs_text = "\n\n".join([
        f"[Index {i}] (p.{doc.metadata.get('page', '?')})\n{doc.page_content[:200]}..."
        for i, doc in enumerate(retrieved_docs[:10])  # ìƒìœ„ 10ê°œë§Œ
    ])
    
    chain = rerank_prompt | llm | StrOutputParser()
    result = chain.invoke({"query": query, "docs_text": docs_text})
    
    # ê²°ê³¼ íŒŒì‹±
    try:
        indices = [int(x.strip()) for x in result.split(',') if x.strip().isdigit()]
        return [retrieved_docs[i] for i in indices if i < len(retrieved_docs)]
    except:
        return retrieved_docs  # íŒŒì‹± ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜


def add_confidence_score(response: str, context_quality: float) -> str:
    """
    LLMì˜ ë‹µë³€ì— ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ í‘œê¸°í•©ë‹ˆë‹¤.
    
    Args:
        response: LLM ë‹µë³€
        context_quality: 0~1 ì‚¬ì´ì˜ ë¬¸ë§¥ í’ˆì§ˆ ì ìˆ˜
    """
    if context_quality > 0.8:
        badge = "âœ… **ë†’ì€ ì‹ ë¢°ë„** (ì¶©ë¶„í•œ ë¬¸ì„œ ê·¼ê±°)"
    elif context_quality > 0.5:
        badge = "âš ï¸ **ì¤‘ê°„ ì‹ ë¢°ë„** (ì œí•œëœ ê·¼ê±°)"
    else:
        badge = "âŒ **ë‚®ì€ ì‹ ë¢°ë„** (ë¶ˆì¶©ë¶„í•œ ê·¼ê±°)"
    
    return f"{badge}\n\n{response}"
